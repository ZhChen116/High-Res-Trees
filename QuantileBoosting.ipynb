{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CN group size: 229\n",
      "AD group size: 187\n",
      "LMCI group size: 401\n",
      "CN 4D tensor shape: (229, 48, 48, 48)\n",
      "CN y shape: (229,)\n",
      "AD 4D tensor shape: (187, 48, 48, 48)\n",
      "AD y shape: (187,)\n",
      "LMCI 4D tensor shape: (401, 48, 48, 48)\n",
      "LMCI y shape: (401,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from skimage.measure import block_reduce\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorly as tl\n",
    "\n",
    "#Debugging import\n",
    "import importlib\n",
    "var = 'TensorDecisionTreeRegressorP' #the published version of code\n",
    "package = importlib.import_module(var)\n",
    "for name, value in package.__dict__.items():\n",
    "    if not name.startswith(\"__\"):\n",
    "        globals()[name] = value\n",
    "\n",
    "from TensorDecisionTreeRegressorP import *\n",
    "\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# File path to the CSV file\n",
    "csv_file = '/Users/zc56/Documents/CommenDesktop/RICE/MyProject/Bayes_Tensor_Tree/3D-images/ADNIData.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Remove rows where ADAS11_bl is missing (NaN)\n",
    "df_cleaned = df.dropna(subset=['ADAS11_bl'])\n",
    "\n",
    "# Extract the 'ADAS11_bl' column as the y variable\n",
    "y_variable = df_cleaned['ADAS11_bl'].values\n",
    "\n",
    "# Split the dataframe based on the DX_bl column values\n",
    "cn_group = df_cleaned[df_cleaned['DX_bl'] == 'CN']\n",
    "ad_group = df_cleaned[df_cleaned['DX_bl'] == 'AD']\n",
    "lmci_group = df_cleaned[df_cleaned['DX_bl'] == 'LMCI']\n",
    "\n",
    "# Display the counts for each group after removing NA\n",
    "print(f\"CN group size: {cn_group.shape[0]}\")\n",
    "print(f\"AD group size: {ad_group.shape[0]}\")\n",
    "print(f\"LMCI group size: {lmci_group.shape[0]}\")\n",
    "\n",
    "# Directory containing the 3D images\n",
    "directory = '/Users/zc56/Documents/CommenDesktop/RICE/MyProject/Bayes_Tensor_Tree/3D-images/3D-Images/bl'\n",
    "\n",
    "# Initialize dictionaries to hold the images and y values for each group\n",
    "cn_images, ad_images, lmci_images = [], [], []\n",
    "cn_y, ad_y, lmci_y = [], [], []\n",
    "\n",
    "# Function to load the images based on PTID matching and append y values\n",
    "def load_images_and_y(group, image_list, y_list):\n",
    "    for _, row in group.iterrows():\n",
    "        ptid = row['PTID']\n",
    "        # Find the corresponding file based on PTID\n",
    "        filename = f'{ptid}.nii.gz'\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            # Load the NIfTI file\n",
    "            img = nib.load(file_path)\n",
    "            data = img.get_fdata()\n",
    "            \n",
    "            # Append the 3D image data and y value to the respective lists\n",
    "            image_list.append(data)\n",
    "            y_list.append(row['ADAS11_bl'])\n",
    "        else:\n",
    "            print(f\"File {filename} not found.\")\n",
    "\n",
    "# Load images and y values for each group\n",
    "load_images_and_y(cn_group, cn_images, cn_y)\n",
    "load_images_and_y(ad_group, ad_images, ad_y)\n",
    "load_images_and_y(lmci_group, lmci_images, lmci_y)\n",
    "\n",
    "# Convert lists of 3D images and y values to NumPy arrays\n",
    "if cn_images:\n",
    "    cn_tensor = np.stack(cn_images, axis=0)\n",
    "    cn_y = np.array(cn_y)\n",
    "    print(f\"CN 4D tensor shape: {cn_tensor.shape}\")\n",
    "    print(f\"CN y shape: {cn_y.shape}\")\n",
    "else:\n",
    "    print(\"No CN images loaded.\")\n",
    "\n",
    "if ad_images:\n",
    "    ad_tensor = np.stack(ad_images, axis=0)\n",
    "    ad_y = np.array(ad_y)\n",
    "    print(f\"AD 4D tensor shape: {ad_tensor.shape}\")\n",
    "    print(f\"AD y shape: {ad_y.shape}\")\n",
    "else:\n",
    "    print(\"No AD images loaded.\")\n",
    "\n",
    "if lmci_images:\n",
    "    lmci_tensor = np.stack(lmci_images, axis=0)\n",
    "    lmci_y = np.array(lmci_y)\n",
    "    print(f\"LMCI 4D tensor shape: {lmci_tensor.shape}\")\n",
    "    print(f\"LMCI y shape: {lmci_y.shape}\")\n",
    "else:\n",
    "    print(\"No LMCI images loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 48, 48, 48) (320,)\n",
      "(320, 12, 12) (320,) (81, 12, 12)\n",
      "mean train RSE:  0.49534330634719587\n",
      "CP train RSE:  0.0022288621342695914\n",
      "Tucker train RSE:  0.0009497448616095284\n",
      "mean test RSE:  1.565813031089088\n",
      "CP test RSE:  3.5873792065584595\n",
      "Tucker test RSE:  3.755752126718517\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(lmci_tensor, lmci_y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(X_train.shape,y_train.shape)\n",
    "model  =  TensorDecisionTreeRegressor(max_depth=6, min_samples_split=4,split_method='variance_LS', split_rank=4, CP_reg_rank=12, Tucker_reg_rank=12, n_mode=3)\n",
    "model.use_mean_as_threshold  =  False\n",
    "model.sample_rate  =  .5\n",
    "X_coarsen_shape = (1,4,4,4)\n",
    "X_coarsen_func = np.max\n",
    "X_train_c = block_reduce(X_train,block_size=X_coarsen_shape, func=X_coarsen_func)\n",
    "X_test_c = block_reduce(X_test,block_size=X_coarsen_shape, func=X_coarsen_func)\n",
    "middle_z = X_train_c.shape[2] // 2\n",
    "X_train_c = X_train_c[:,:,:,middle_z]\n",
    "X_test_c = X_test_c[:,:,:,middle_z]\n",
    "X_train_c = X_train_c+np.ones_like(X_train_c)*1e-3\n",
    "print(X_train_c.shape,y_train.shape,X_test_c.shape)\n",
    "model.fit(X_train_c,y_train)\n",
    "\n",
    "predictions = model.predict(X_train_c,regression_method='mean')\n",
    "print(f\"mean train RSE: \", np.mean((predictions-y_train)**2)/np.var(y_train))\n",
    "predictions = model.predict(X_train_c,regression_method='cp')\n",
    "print(f\"CP train RSE: \", np.mean((predictions-y_train)**2)/np.var(y_train))\n",
    "predictions = model.predict(X_train_c,regression_method='tucker')\n",
    "print(f\"Tucker train RSE: \", np.mean((predictions-y_train)**2)/np.var(y_train)) \n",
    "\n",
    "predictions = model.predict(X_test_c,regression_method='mean')\n",
    "print(f\"mean test RSE: \", np.mean((predictions-y_test)**2)/np.var(y_test))\n",
    "predictions = model.predict(X_test_c,regression_method='cp')\n",
    "print(f\"CP test RSE: \", np.mean((predictions-y_test)**2)/np.var(y_test))\n",
    "predictions = model.predict(X_test_c,regression_method='tucker')\n",
    "print(f\"Tucker test RSE: \", np.mean((predictions-y_test)**2)/np.var(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting started\n",
      "0 0 training RMSE === 7.737706822618623\n",
      "0 0 testing RMSE === 12.996566526626966\n",
      "0 1 training RMSE === 5.089903764622437\n",
      "0 1 testing RMSE === 9.283254428248284\n",
      "0 2 training RMSE === 3.209273023968686\n",
      "0 2 testing RMSE === 6.682373109499321\n",
      "0 3 training RMSE === 1.9061587152196653\n",
      "0 3 testing RMSE === 4.778812791726211\n",
      "0 4 training RMSE === 1.067546514427356\n",
      "0 4 testing RMSE === 3.4896072321320477\n",
      "0 5 training RMSE === 0.5513836012617743\n",
      "0 5 testing RMSE === 2.6069779248447658\n",
      "0 6 training RMSE === 0.3173098395858351\n",
      "0 6 testing RMSE === 2.080529151064152\n",
      "0 7 training RMSE === 0.2630594934991479\n",
      "0 7 testing RMSE === 1.7933619842373365\n",
      "0 8 training RMSE === 0.3561509254349389\n",
      "0 8 testing RMSE === 1.7057151181285752\n",
      "0 9 training RMSE === 0.5576546712729897\n",
      "0 9 testing RMSE === 1.7542303101548022\n",
      "0 10 training RMSE === 0.8282283569393364\n",
      "0 10 testing RMSE === 1.8994492826923746\n",
      "0 11 training RMSE === 1.1436765205613892\n",
      "0 11 testing RMSE === 2.1419012879630075\n",
      "0 12 training RMSE === 1.4925128687586524\n",
      "0 12 testing RMSE === 2.436989417349509\n",
      "0 13 training RMSE === 1.856868377339583\n",
      "0 13 testing RMSE === 2.795651720223555\n",
      "0 14 training RMSE === 2.2378354300788157\n",
      "0 14 testing RMSE === 3.1675863195495135\n",
      "0 15 training RMSE === 2.6082617344978547\n",
      "0 15 testing RMSE === 3.5499516000092046\n",
      "0 16 training RMSE === 2.9723508953644067\n",
      "0 16 testing RMSE === 3.8991726479077036\n",
      "0 17 training RMSE === 3.324102892354888\n",
      "0 17 testing RMSE === 4.275662771881467\n",
      "0 18 training RMSE === 3.6659110763284937\n",
      "0 18 testing RMSE === 4.612008836457415\n",
      "0 19 training RMSE === 3.9875430610774756\n",
      "0 19 testing RMSE === 4.973229218214663\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Regressor Class\n",
    "from GradientBoosting import *\n",
    "# Initialize Gradient Boosting Regressor with a simple decision tree as weak learner\n",
    "#weak_learner = TensorDecisionTreeRegressor(max_depth=3, min_samples_split=4, split_method='variance',split_rank=my_rank,n_mode=my_n_mode)\n",
    "#weak_learner.use_mean_as_threshold = True\n",
    "#weak_learner.sample_rate = 1.0\n",
    "weak_learner = model\n",
    "gradient_boosting_regressor = GradientBoostingRegressor(\n",
    "    n_estimators=20,\n",
    "    learning_rate=0.1,\n",
    "    weak_learner=weak_learner\n",
    ")\n",
    "#gradient_boosting_regressor.pruning = True\n",
    "# Fit a single tree model\n",
    "weak_learner.fit(X_train_c, y_train)\n",
    "weak_predictions = weak_learner.predict(X_test_c)\n",
    "\n",
    "\n",
    "# Fit the Gradient Boosting model\n",
    "gradient_boosting_regressor.fit(X_train_c, y_train, X_test_c, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Gradient Boosting Regressor Class for LAD_TreeBoost\n",
    "class LAD_TreeBoost:\n",
    "    def __init__(self, n_estimators, learning_rate, weak_learner, n_iterations=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weak_learner = weak_learner\n",
    "        self.models = [deepcopy(self.weak_learner) for _ in range(n_estimators)]\n",
    "        self.initial_model = None\n",
    "        self.n_iterations = n_iterations\n",
    "        self.pruning = False\n",
    "\n",
    "    def fit(self, X, y, X_test=None, y_test=None):\n",
    "        # Initialize the model with the median of y (LAD initialization)\n",
    "        self.initial_model = self.initialize_model(y)\n",
    "        current_pred = self.initial_model * np.ones(shape=y.shape)\n",
    "        print(\"Fitting started\")\n",
    "\n",
    "        # Iteratively add weak learners\n",
    "        for j in range(self.n_iterations):\n",
    "            for i in range(self.n_estimators):\n",
    "                # Calculate predictions of the current model\n",
    "                current_pred = self.predict(X)\n",
    "\n",
    "                # Calculate residual: pseudo-response for LAD is y - F_{m-1}(x)\n",
    "                residual = y - current_pred\n",
    "                print(f\"Iteration {j}, Estimator {i}: Training LAD Loss = {np.mean(np.abs(residual))}\")\n",
    "                \n",
    "                # Optional: Testing performance\n",
    "                if X_test is not None and y_test is not None:\n",
    "                    current_pred_test = self.predict(X_test)\n",
    "                    residual_test = y_test - current_pred_test\n",
    "                    print(f\"Iteration {j}, Estimator {i}: Testing LAD Loss = {np.mean(np.abs(residual_test))}\")\n",
    "\n",
    "                # Fit weak learner to residual (pseudo-response)\n",
    "                self.models[i].fit(X, residual)\n",
    "                if self.pruning:\n",
    "                    self.models[i].prune()\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Start with initial model prediction (constant median value)\n",
    "        y_pred = self.initial_model * np.ones(X.shape[0])\n",
    "\n",
    "        # Add predictions from all weak learners\n",
    "        for learner in self.models:\n",
    "            learner_pred = learner.predict(X)\n",
    "            if learner_pred is not None:\n",
    "                # Update the prediction: Add gamma (median of predictions) for LAD\n",
    "                y_pred += self.learning_rate * learner_pred\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def initialize_model(self, y):\n",
    "        # Initialize with the median of y for LAD\n",
    "        return np.median(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 48, 48, 48) (320,)\n",
      "(320, 12, 12) (320,) (81, 12, 12)\n",
      "mean train RSE:  0.36301627948486365\n",
      "CP train RSE:  0.0013483275523180187\n",
      "Tucker train RSE:  0.0007197489649219131\n",
      "mean test RSE:  1.9043718496234767\n",
      "CP test RSE:  2.7561558637589\n",
      "Tucker test RSE:  2.977140839216029\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(lmci_tensor, lmci_y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(X_train.shape,y_train.shape)\n",
    "model  =  TensorDecisionTreeRegressor(max_depth=6, min_samples_split=4,split_method='variance_LS', split_rank=4, CP_reg_rank=12, Tucker_reg_rank=12, n_mode=3)\n",
    "model.use_mean_as_threshold  =  False\n",
    "model.sample_rate  =  .5\n",
    "X_coarsen_shape = (1,4,4,4)\n",
    "X_coarsen_func = np.max\n",
    "X_train_c = block_reduce(X_train,block_size=X_coarsen_shape, func=X_coarsen_func)\n",
    "X_test_c = block_reduce(X_test,block_size=X_coarsen_shape, func=X_coarsen_func)\n",
    "middle_z = X_train_c.shape[2] // 2\n",
    "X_train_c = X_train_c[:,:,:,middle_z]\n",
    "X_test_c = X_test_c[:,:,:,middle_z]\n",
    "print(X_train_c.shape,y_train.shape,X_test_c.shape)\n",
    "\n",
    "\n",
    "noise_level = 1e-2\n",
    "X_train_c = X_train_c+noise_level * np.random.randn(*X_train_c.shape)\n",
    "X_test_c = X_test_c+noise_level * np.random.randn(*X_test_c.shape)\n",
    "model.fit(X_train_c,y_train)\n",
    "\n",
    "predictions = model.predict(X_train_c,regression_method='mean')\n",
    "print(f\"mean train RSE: \", np.mean((predictions-y_train)**2)/np.var(y_train))\n",
    "predictions = model.predict(X_train_c,regression_method='cp')\n",
    "print(f\"CP train RSE: \", np.mean((predictions-y_train)**2)/np.var(y_train))\n",
    "predictions = model.predict(X_train_c,regression_method='tucker')\n",
    "print(f\"Tucker train RSE: \", np.mean((predictions-y_train)**2)/np.var(y_train)) \n",
    "\n",
    "predictions = model.predict(X_test_c,regression_method='mean')\n",
    "print(f\"mean test RSE: \", np.mean((predictions-y_test)**2)/np.var(y_test))\n",
    "predictions = model.predict(X_test_c,regression_method='cp')\n",
    "print(f\"CP test RSE: \", np.mean((predictions-y_test)**2)/np.var(y_test))\n",
    "predictions = model.predict(X_test_c,regression_method='tucker')\n",
    "print(f\"Tucker test RSE: \", np.mean((predictions-y_test)**2)/np.var(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting started\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Instantiate and train LAD_TreeBoost\u001b[39;00m\n\u001b[1;32m     13\u001b[0m lad_boost \u001b[38;5;241m=\u001b[39m LAD_TreeBoost(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, weak_learner\u001b[38;5;241m=\u001b[39mweak_learner)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mlad_boost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Make predictions on the test set\u001b[39;00m\n\u001b[1;32m     17\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m lad_boost\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m, in \u001b[0;36mLAD_TreeBoost.fit\u001b[0;34m(self, X, y, X_test, y_test)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iterations):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators):\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;66;03m# Calculate predictions of the current model\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m         current_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# Calculate residual: pseudo-response for LAD is y - F_{m-1}(x)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         residual \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m-\u001b[39m current_pred\n",
      "Cell \u001b[0;32mIn[2], line 50\u001b[0m, in \u001b[0;36mLAD_TreeBoost.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Add predictions from all weak learners\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m learner \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels:\n\u001b[0;32m---> 50\u001b[0m     learner_pred \u001b[38;5;241m=\u001b[39m \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m learner_pred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;66;03m# Update the prediction: Add gamma (median of predictions) for LAD\u001b[39;00m\n\u001b[1;32m     53\u001b[0m         y_pred \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m learner_pred\n",
      "File \u001b[0;32m~/Documents/CommenDesktop/RICE/MyProject/Bayes_Tensor_Tree/serverBTR/TensorDecisionTreeRegressorP.py:533\u001b[0m, in \u001b[0;36mTensorDecisionTreeRegressor.predict\u001b[0;34m(self, X, regression_method)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, regression_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;66;03m#print('predict->regression_method:',regression_method)\u001b[39;00m\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m regression_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 533\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_traverse_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X])\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m regression_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    535\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_traverse_tree_with_cp_regression(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X])\n",
      "File \u001b[0;32m~/Documents/CommenDesktop/RICE/MyProject/Bayes_Tensor_Tree/serverBTR/TensorDecisionTreeRegressorP.py:571\u001b[0m, in \u001b[0;36mTensorDecisionTreeRegressor._traverse_tree\u001b[0;34m(self, x, node, return_leaf_index)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_traverse_tree(x, node\u001b[38;5;241m.\u001b[39mright, return_leaf_index)\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x[node\u001b[38;5;241m.\u001b[39mfeature_index[\u001b[38;5;241m0\u001b[39m], node\u001b[38;5;241m.\u001b[39mfeature_index[\u001b[38;5;241m1\u001b[39m], \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mthreshold:\n\u001b[1;32m    572\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_traverse_tree(x, node\u001b[38;5;241m.\u001b[39mleft, return_leaf_index)\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "weak_learner = model\n",
    "gradient_boosting_regressor = LAD_TreeBoost(\n",
    "    n_estimators=10,\n",
    "    learning_rate=0.1,\n",
    "    weak_learner=weak_learner\n",
    ")\n",
    "#gradient_boosting_regressor.pruning = True\n",
    "# Fit a single tree model\n",
    "weak_learner.fit(X_train_c, y_train)\n",
    "weak_predictions = weak_learner.predict(X_test_c)\n",
    "\n",
    "# Instantiate and train LAD_TreeBoost\n",
    "lad_boost = LAD_TreeBoost(n_estimators=100, learning_rate=0.1, weak_learner=weak_learner)\n",
    "lad_boost.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = lad_boost.predict(X_test)\n",
    "\n",
    "# Calculate the Relative Absolute Error (RAE)\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "rae_test = mean_absolute_error(y_test, y_pred) / np.mean(np.abs(y_test - np.median(y_test)))\n",
    "\n",
    "print(\"Testing Relative Absolute Error (RAE):\", rae_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
